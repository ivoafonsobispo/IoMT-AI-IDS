{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_T2LCfAN_XF"
   },
   "source": [
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1li4zpPN_XH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0t7965zN_XH"
   },
   "source": [
    "Panda Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "brXljTxkN_XI"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoPy7wczN_XI"
   },
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YM-QslAnN_XI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>traffic</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>total_pkts</th>\n",
       "      <th>pkt_difference</th>\n",
       "      <th>byte_difference</th>\n",
       "      <th>total_data_pkts</th>\n",
       "      <th>payload_ratio</th>\n",
       "      <th>total_payload_volume</th>\n",
       "      <th>fwd_bwd_pkts_diff</th>\n",
       "      <th>duration_weighted_pkts</th>\n",
       "      <th>pkts_size_weighted</th>\n",
       "      <th>flow_pkts_size_weighted</th>\n",
       "      <th>header_size_ratio</th>\n",
       "      <th>total_header_size</th>\n",
       "      <th>header_size_diff</th>\n",
       "      <th>fwd_bwd_payload_tot_diff</th>\n",
       "      <th>fwd_bwd_payload_avg_diff</th>\n",
       "      <th>flow_fwd_payload_diff</th>\n",
       "      <th>flow_bwd_payload_diff</th>\n",
       "      <th>flow_payload_range</th>\n",
       "      <th>total_activity</th>\n",
       "      <th>history_originator</th>\n",
       "      <th>history_responder</th>\n",
       "      <th>proto_1</th>\n",
       "      <th>proto_2</th>\n",
       "      <th>proto_3</th>\n",
       "      <th>pkts_unidirectional_traffic_0</th>\n",
       "      <th>pkts_unidirectional_traffic_1</th>\n",
       "      <th>iat_is_unidirectional_False</th>\n",
       "      <th>iat_is_unidirectional_True</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rudeadyet</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.083659</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>-0.068478</td>\n",
       "      <td>0.272657</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.282959</td>\n",
       "      <td>0.275477</td>\n",
       "      <td>0.072084</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427032</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>10</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.991445</td>\n",
       "      <td>-0.033842</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.277062</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.177140</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.420676</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>6</td>\n",
       "      <td>0.465697</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>6</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>1.228979</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.009351</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.020961</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>-0.148246</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>-0.241610</td>\n",
       "      <td>1.491583</td>\n",
       "      <td>1.067853</td>\n",
       "      <td>-0.042359</td>\n",
       "      <td>-0.426966</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>netscan</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.025583</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.275309</td>\n",
       "      <td>-0.183556</td>\n",
       "      <td>-0.257187</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>6</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>1.228979</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.009273</td>\n",
       "      <td>0.020662</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>-0.148246</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>-0.241610</td>\n",
       "      <td>1.491583</td>\n",
       "      <td>1.067853</td>\n",
       "      <td>-0.042359</td>\n",
       "      <td>-0.426964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439302</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>10</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.991444</td>\n",
       "      <td>-0.033842</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.277062</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.177140</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.394997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439303</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.463950</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>3.474099</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>3.458871</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.395117</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439304</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>10</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.991444</td>\n",
       "      <td>-0.033842</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.277062</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.177140</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.420131</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439305</th>\n",
       "      <td>0</td>\n",
       "      <td>camoverflow</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.463950</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.288748</td>\n",
       "      <td>-0.571970</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439306</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>1</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>-0.145528</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.463950</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.194676</td>\n",
       "      <td>-0.501349</td>\n",
       "      <td>-0.210149</td>\n",
       "      <td>3.667838</td>\n",
       "      <td>9.808385</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>1.987301</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439307 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        service      traffic  total_bytes  total_pkts  pkt_difference  \\\n",
       "0             0    rudeadyet    -0.004437           3       -0.083659   \n",
       "1             2       normal    -0.004434          10        0.034060   \n",
       "2             0       normal    -0.004383           6        0.465697   \n",
       "3             0      netscan    -0.004437           2       -0.122899   \n",
       "4             0       normal    -0.004437           6       -0.122899   \n",
       "...         ...          ...          ...         ...             ...   \n",
       "439302        2       normal    -0.004434          10        0.073300   \n",
       "439303        0       normal    -0.004437          10       -0.122899   \n",
       "439304        2       normal    -0.004434          10        0.073300   \n",
       "439305        0  camoverflow    -0.004437           1       -0.122899   \n",
       "439306        0       normal    -0.004437           1       -0.122899   \n",
       "\n",
       "        byte_difference  total_data_pkts  payload_ratio  total_payload_volume  \\\n",
       "0             -0.003766                0      -0.558121             -0.420430   \n",
       "1             -0.003763                5       0.972797              0.954078   \n",
       "2             -0.003696                6       2.503716              1.228979   \n",
       "3             -0.003766                0      -0.558121             -0.420430   \n",
       "4             -0.003766                6       2.503716              1.228979   \n",
       "...                 ...              ...            ...                   ...   \n",
       "439302        -0.003762                5       0.972797              0.954078   \n",
       "439303        -0.003766                5       0.972797              0.954078   \n",
       "439304        -0.003762                5       0.972797              0.954078   \n",
       "439305        -0.003766                0      -0.558121             -0.420430   \n",
       "439306        -0.003766                1       2.503716             -0.145528   \n",
       "\n",
       "        fwd_bwd_pkts_diff  duration_weighted_pkts  pkts_size_weighted  \\\n",
       "0               -0.068478                0.272657           -0.033846   \n",
       "1                0.000492                1.991445           -0.033842   \n",
       "2                0.000492                1.009351            0.020996   \n",
       "3                0.000492                0.025583           -0.033846   \n",
       "4                0.000492                1.009273            0.020662   \n",
       "...                   ...                     ...                 ...   \n",
       "439302           0.000492                1.991444           -0.033842   \n",
       "439303           0.000492               -0.463950           -0.033846   \n",
       "439304           0.000492                1.991444           -0.033842   \n",
       "439305           0.000492               -0.463950           -0.033846   \n",
       "439306           0.000492               -0.463950           -0.033846   \n",
       "\n",
       "        flow_pkts_size_weighted  header_size_ratio  total_header_size  \\\n",
       "0                     -0.033855          -0.282959           0.275477   \n",
       "1                     -0.033850          -0.277062           2.252854   \n",
       "2                      0.020961          -0.277459          -0.148246   \n",
       "3                     -0.033855          -0.275309          -0.183556   \n",
       "4                      0.020627          -0.277459          -0.148246   \n",
       "...                         ...                ...                ...   \n",
       "439302                -0.033850          -0.277062           2.252854   \n",
       "439303                -0.033855           3.474099           2.252854   \n",
       "439304                -0.033850          -0.277062           2.252854   \n",
       "439305                -0.033855          -0.288748          -0.571970   \n",
       "439306                -0.033855          -0.194676          -0.501349   \n",
       "\n",
       "        header_size_diff  fwd_bwd_payload_tot_diff  fwd_bwd_payload_avg_diff  \\\n",
       "0               0.072084                 -0.190720                 -0.181788   \n",
       "1              -0.304226                  0.502434                  0.177140   \n",
       "2              -0.304226                 -0.260035                 -0.241610   \n",
       "3              -0.257187                 -0.190720                 -0.181788   \n",
       "4              -0.304226                 -0.260035                 -0.241610   \n",
       "...                  ...                       ...                       ...   \n",
       "439302         -0.304226                  0.502434                  0.177140   \n",
       "439303          3.458871                  0.502434                 -0.002324   \n",
       "439304         -0.304226                  0.502434                  0.177140   \n",
       "439305         -0.304226                 -0.190720                 -0.181788   \n",
       "439306         -0.210149                  3.667838                  9.808385   \n",
       "\n",
       "        flow_fwd_payload_diff  flow_bwd_payload_diff  flow_payload_range  \\\n",
       "0                   -0.233301              -0.304458           -0.147905   \n",
       "1                   -0.233301               0.107235            0.063188   \n",
       "2                    1.491583               1.067853           -0.042359   \n",
       "3                   -0.233301              -0.304458           -0.147905   \n",
       "4                    1.491583               1.067853           -0.042359   \n",
       "...                       ...                    ...                 ...   \n",
       "439302              -0.233301               0.107235            0.063188   \n",
       "439303              -0.233301               0.107235            0.063188   \n",
       "439304              -0.233301               0.107235            0.063188   \n",
       "439305              -0.233301              -0.304458           -0.147905   \n",
       "439306              -0.233301               1.987301           -0.147905   \n",
       "\n",
       "        total_activity  history_originator  history_responder  proto_1  \\\n",
       "0            -0.427032                   1                  1        1   \n",
       "1             2.420676                   1                  1        1   \n",
       "2            -0.426966                   1                  2        0   \n",
       "3            -0.427041                   1                  1        1   \n",
       "4            -0.426964                   1                  1        1   \n",
       "...                ...                 ...                ...      ...   \n",
       "439302        2.394997                   1                  1        1   \n",
       "439303        2.395117                   1                  1        1   \n",
       "439304        2.420131                   1                  1        1   \n",
       "439305       -0.427041                   0                  0        1   \n",
       "439306       -0.427041                   1                  1        1   \n",
       "\n",
       "        proto_2  proto_3  pkts_unidirectional_traffic_0  \\\n",
       "0             0        0                              0   \n",
       "1             0        0                              0   \n",
       "2             1        0                              0   \n",
       "3             0        0                              1   \n",
       "4             0        0                              0   \n",
       "...         ...      ...                            ...   \n",
       "439302        0        0                              0   \n",
       "439303        0        0                              0   \n",
       "439304        0        0                              0   \n",
       "439305        0        0                              0   \n",
       "439306        0        0                              0   \n",
       "\n",
       "        pkts_unidirectional_traffic_1  iat_is_unidirectional_False  \\\n",
       "0                                   1                            1   \n",
       "1                                   1                            1   \n",
       "2                                   1                            1   \n",
       "3                                   0                            0   \n",
       "4                                   1                            1   \n",
       "...                               ...                          ...   \n",
       "439302                              1                            1   \n",
       "439303                              1                            0   \n",
       "439304                              1                            1   \n",
       "439305                              1                            0   \n",
       "439306                              1                            0   \n",
       "\n",
       "        iat_is_unidirectional_True  is_attack  \n",
       "0                                0          1  \n",
       "1                                0          0  \n",
       "2                                0          0  \n",
       "3                                1          1  \n",
       "4                                0          0  \n",
       "...                            ...        ...  \n",
       "439302                           0          0  \n",
       "439303                           1          0  \n",
       "439304                           0          0  \n",
       "439305                           1          1  \n",
       "439306                           1          0  \n",
       "\n",
       "[439307 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../../../../Datasets/Flows/train.csv\")\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>traffic</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>total_pkts</th>\n",
       "      <th>pkt_difference</th>\n",
       "      <th>byte_difference</th>\n",
       "      <th>total_data_pkts</th>\n",
       "      <th>payload_ratio</th>\n",
       "      <th>total_payload_volume</th>\n",
       "      <th>fwd_bwd_pkts_diff</th>\n",
       "      <th>duration_weighted_pkts</th>\n",
       "      <th>pkts_size_weighted</th>\n",
       "      <th>flow_pkts_size_weighted</th>\n",
       "      <th>header_size_ratio</th>\n",
       "      <th>total_header_size</th>\n",
       "      <th>header_size_diff</th>\n",
       "      <th>fwd_bwd_payload_tot_diff</th>\n",
       "      <th>fwd_bwd_payload_avg_diff</th>\n",
       "      <th>flow_fwd_payload_diff</th>\n",
       "      <th>flow_bwd_payload_diff</th>\n",
       "      <th>flow_payload_range</th>\n",
       "      <th>total_activity</th>\n",
       "      <th>history_originator</th>\n",
       "      <th>history_responder</th>\n",
       "      <th>proto_1</th>\n",
       "      <th>proto_2</th>\n",
       "      <th>proto_3</th>\n",
       "      <th>pkts_unidirectional_traffic_0</th>\n",
       "      <th>pkts_unidirectional_traffic_1</th>\n",
       "      <th>iat_is_unidirectional_False</th>\n",
       "      <th>iat_is_unidirectional_True</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>6</td>\n",
       "      <td>0.465697</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>6</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>1.228979</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.009201</td>\n",
       "      <td>0.028962</td>\n",
       "      <td>0.028923</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>-0.148246</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>-0.241610</td>\n",
       "      <td>1.491583</td>\n",
       "      <td>1.067853</td>\n",
       "      <td>-0.042359</td>\n",
       "      <td>-0.426968</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>6</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>6</td>\n",
       "      <td>2.503716</td>\n",
       "      <td>1.228979</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.009343</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>-0.148246</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>-0.241610</td>\n",
       "      <td>1.491583</td>\n",
       "      <td>1.067853</td>\n",
       "      <td>-0.042359</td>\n",
       "      <td>-0.426950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>netscan</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.026684</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.275309</td>\n",
       "      <td>-0.183556</td>\n",
       "      <td>-0.257187</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>8</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>4</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.679176</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.500366</td>\n",
       "      <td>-0.033842</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.277080</td>\n",
       "      <td>1.687889</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>0.363804</td>\n",
       "      <td>0.177140</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>1.188589</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>10</td>\n",
       "      <td>0.465697</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.463950</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>3.474099</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>3.458871</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.392148</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146431</th>\n",
       "      <td>0</td>\n",
       "      <td>rudeadyet</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.083659</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>-0.018061</td>\n",
       "      <td>0.272664</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.282959</td>\n",
       "      <td>0.275477</td>\n",
       "      <td>0.072084</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.426917</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146432</th>\n",
       "      <td>0</td>\n",
       "      <td>slowloris</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.083659</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>-0.526904</td>\n",
       "      <td>0.272717</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.282959</td>\n",
       "      <td>0.275477</td>\n",
       "      <td>0.072084</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146433</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>7</td>\n",
       "      <td>1.090560</td>\n",
       "      <td>1.503881</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.463950</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>4.602953</td>\n",
       "      <td>3.100301</td>\n",
       "      <td>4.587800</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>-0.025332</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.162128</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.391199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146434</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954078</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>1.991443</td>\n",
       "      <td>-0.033842</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.277062</td>\n",
       "      <td>2.252854</td>\n",
       "      <td>-0.304226</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.177140</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>0.063188</td>\n",
       "      <td>2.398506</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146435</th>\n",
       "      <td>0</td>\n",
       "      <td>netscan</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.122899</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558121</td>\n",
       "      <td>-0.420430</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.026463</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.275309</td>\n",
       "      <td>-0.183556</td>\n",
       "      <td>-0.257187</td>\n",
       "      <td>-0.190720</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>-0.233301</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.147905</td>\n",
       "      <td>-0.427041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146436 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        service    traffic  total_bytes  total_pkts  pkt_difference  \\\n",
       "0             0     normal    -0.004383           6        0.465697   \n",
       "1             2     normal    -0.004434           6        0.073300   \n",
       "2             0    netscan    -0.004437           2       -0.122899   \n",
       "3             2     normal    -0.004434           8        0.073300   \n",
       "4             0     normal    -0.004383          10        0.465697   \n",
       "...         ...        ...          ...         ...             ...   \n",
       "146431        0  rudeadyet    -0.004437           3       -0.083659   \n",
       "146432        0  slowloris    -0.004437           3       -0.083659   \n",
       "146433        0     normal    -0.004437          13       -0.122899   \n",
       "146434        0     normal    -0.004437          10       -0.122899   \n",
       "146435        0    netscan    -0.004437           2       -0.122899   \n",
       "\n",
       "        byte_difference  total_data_pkts  payload_ratio  total_payload_volume  \\\n",
       "0             -0.003696                6       2.503716              1.228979   \n",
       "1             -0.003762                6       2.503716              1.228979   \n",
       "2             -0.003766                0      -0.558121             -0.420430   \n",
       "3             -0.003762                4       0.972797              0.679176   \n",
       "4             -0.003696                5       0.972797              0.954078   \n",
       "...                 ...              ...            ...                   ...   \n",
       "146431        -0.003766                0      -0.558121             -0.420430   \n",
       "146432        -0.003766                0      -0.558121             -0.420430   \n",
       "146433        -0.003766                7       1.090560              1.503881   \n",
       "146434        -0.003766                5       0.972797              0.954078   \n",
       "146435        -0.003766                0      -0.558121             -0.420430   \n",
       "\n",
       "        fwd_bwd_pkts_diff  duration_weighted_pkts  pkts_size_weighted  \\\n",
       "0                0.000492                1.009201            0.028962   \n",
       "1                0.000492                1.009343            0.021669   \n",
       "2                0.000492                0.026684           -0.033846   \n",
       "3                0.000492                1.500366           -0.033842   \n",
       "4                0.000492               -0.463950           -0.033846   \n",
       "...                   ...                     ...                 ...   \n",
       "146431          -0.018061                0.272664           -0.033846   \n",
       "146432          -0.526904                0.272717           -0.033846   \n",
       "146433           0.000492               -0.463950           -0.033846   \n",
       "146434           0.000492                1.991443           -0.033842   \n",
       "146435           0.000492                0.026463           -0.033846   \n",
       "\n",
       "        flow_pkts_size_weighted  header_size_ratio  total_header_size  \\\n",
       "0                      0.028923          -0.277459          -0.148246   \n",
       "1                      0.021634          -0.277459          -0.148246   \n",
       "2                     -0.033855          -0.275309          -0.183556   \n",
       "3                     -0.033850          -0.277080           1.687889   \n",
       "4                     -0.033855           3.474099           2.252854   \n",
       "...                         ...                ...                ...   \n",
       "146431                -0.033855          -0.282959           0.275477   \n",
       "146432                -0.033855          -0.282959           0.275477   \n",
       "146433                -0.033855           4.602953           3.100301   \n",
       "146434                -0.033850          -0.277062           2.252854   \n",
       "146435                -0.033855          -0.275309          -0.183556   \n",
       "\n",
       "        header_size_diff  fwd_bwd_payload_tot_diff  fwd_bwd_payload_avg_diff  \\\n",
       "0              -0.304226                 -0.260035                 -0.241610   \n",
       "1              -0.304226                 -0.260035                 -0.241610   \n",
       "2              -0.257187                 -0.190720                 -0.181788   \n",
       "3              -0.304226                  0.363804                  0.177140   \n",
       "4               3.458871                  0.502434                 -0.002324   \n",
       "...                  ...                       ...                       ...   \n",
       "146431          0.072084                 -0.190720                 -0.181788   \n",
       "146432          0.072084                 -0.190720                 -0.181788   \n",
       "146433          4.587800                  0.594855                 -0.025332   \n",
       "146434         -0.304226                  0.502434                  0.177140   \n",
       "146435         -0.257187                 -0.190720                 -0.181788   \n",
       "\n",
       "        flow_fwd_payload_diff  flow_bwd_payload_diff  flow_payload_range  \\\n",
       "0                    1.491583               1.067853           -0.042359   \n",
       "1                    1.491583               1.067853           -0.042359   \n",
       "2                   -0.233301              -0.304458           -0.147905   \n",
       "3                   -0.233301               0.024896            0.063188   \n",
       "4                   -0.233301               0.107235            0.063188   \n",
       "...                       ...                    ...                 ...   \n",
       "146431              -0.233301              -0.304458           -0.147905   \n",
       "146432              -0.233301              -0.304458           -0.147905   \n",
       "146433              -0.233301               0.162128            0.063188   \n",
       "146434              -0.233301               0.107235            0.063188   \n",
       "146435              -0.233301              -0.304458           -0.147905   \n",
       "\n",
       "        total_activity  history_originator  history_responder  proto_1  \\\n",
       "0            -0.426968                   1                  2        0   \n",
       "1            -0.426950                   1                  1        1   \n",
       "2            -0.427041                   1                  1        1   \n",
       "3             1.188589                   1                  1        1   \n",
       "4             2.392148                   1                  2        0   \n",
       "...                ...                 ...                ...      ...   \n",
       "146431       -0.426917                   1                  1        1   \n",
       "146432       -0.427041                   1                  1        1   \n",
       "146433        2.391199                   1                  1        1   \n",
       "146434        2.398506                   1                  1        1   \n",
       "146435       -0.427041                   1                  1        1   \n",
       "\n",
       "        proto_2  proto_3  pkts_unidirectional_traffic_0  \\\n",
       "0             1        0                              0   \n",
       "1             0        0                              0   \n",
       "2             0        0                              1   \n",
       "3             0        0                              0   \n",
       "4             1        0                              0   \n",
       "...         ...      ...                            ...   \n",
       "146431        0        0                              0   \n",
       "146432        0        0                              0   \n",
       "146433        0        0                              0   \n",
       "146434        0        0                              0   \n",
       "146435        0        0                              1   \n",
       "\n",
       "        pkts_unidirectional_traffic_1  iat_is_unidirectional_False  \\\n",
       "0                                   1                            1   \n",
       "1                                   1                            1   \n",
       "2                                   0                            0   \n",
       "3                                   1                            1   \n",
       "4                                   1                            0   \n",
       "...                               ...                          ...   \n",
       "146431                              1                            1   \n",
       "146432                              1                            1   \n",
       "146433                              1                            0   \n",
       "146434                              1                            1   \n",
       "146435                              0                            0   \n",
       "\n",
       "        iat_is_unidirectional_True  is_attack  \n",
       "0                                0          0  \n",
       "1                                0          0  \n",
       "2                                1          1  \n",
       "3                                0          0  \n",
       "4                                1          0  \n",
       "...                            ...        ...  \n",
       "146431                           0          1  \n",
       "146432                           0          1  \n",
       "146433                           1          0  \n",
       "146434                           0          0  \n",
       "146435                           1          1  \n",
       "\n",
       "[146436 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../../../../../Datasets/Flows/test.csv\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcHY3Kk_N_XN"
   },
   "source": [
    "---------------------------------------\n",
    "\n",
    "**Create Model & Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j9A5Vaq8Naa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "traffic\n",
       "apachekiller      1756\n",
       "arpspoofing       8361\n",
       "camoverflow      46934\n",
       "mqttmalaria       3709\n",
       "netscan          46767\n",
       "normal          225000\n",
       "rudeadyet        54088\n",
       "slowloris        47685\n",
       "slowread          5007\n",
       "Name: traffic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('traffic')['traffic'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('is_attack', axis=1)\n",
    "df_test = df_test.drop('is_attack', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g7ExpPs6N_XN"
   },
   "outputs": [],
   "source": [
    "x_columns = df_train.columns.drop('traffic')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train[\"traffic\"].values)\n",
    "\n",
    "x = df_train[x_columns].values\n",
    "y = df_train[\"traffic\"].values\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns_val = df_test.columns.drop('traffic')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_test[\"traffic\"].values)\n",
    "\n",
    "x_val = df_test[x_columns].values\n",
    "y_val = df_test[\"traffic\"].values\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JllGCOC76fxm"
   },
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((109827, 30), (109827,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((329480, 30), (329480,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kmWyRAR0N_XN",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 329480 samples, validate on 109827 samples\n",
      "Epoch 1/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 1.0739 - accuracy: 0.6844 - val_loss: 0.3337 - val_accuracy: 0.9118\n",
      "Epoch 2/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.5027 - accuracy: 0.8503 - val_loss: 0.2805 - val_accuracy: 0.9163\n",
      "Epoch 3/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.4181 - accuracy: 0.8768 - val_loss: 0.2529 - val_accuracy: 0.9340\n",
      "Epoch 4/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.3726 - accuracy: 0.8918 - val_loss: 0.2312 - val_accuracy: 0.9377\n",
      "Epoch 5/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.3420 - accuracy: 0.9021 - val_loss: 0.2152 - val_accuracy: 0.9373\n",
      "Epoch 6/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.3180 - accuracy: 0.9101 - val_loss: 0.2024 - val_accuracy: 0.9380\n",
      "Epoch 7/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2972 - accuracy: 0.9165 - val_loss: 0.1930 - val_accuracy: 0.9444\n",
      "Epoch 8/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2807 - accuracy: 0.9218 - val_loss: 0.1866 - val_accuracy: 0.9458\n",
      "Epoch 9/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2683 - accuracy: 0.9260 - val_loss: 0.1815 - val_accuracy: 0.9462\n",
      "Epoch 10/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2575 - accuracy: 0.9288 - val_loss: 0.1773 - val_accuracy: 0.9477\n",
      "Epoch 11/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2483 - accuracy: 0.9312 - val_loss: 0.1745 - val_accuracy: 0.9480\n",
      "Epoch 12/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2398 - accuracy: 0.9337 - val_loss: 0.1688 - val_accuracy: 0.9493\n",
      "Epoch 13/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2335 - accuracy: 0.9351 - val_loss: 0.1684 - val_accuracy: 0.9511\n",
      "Epoch 14/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2267 - accuracy: 0.9369 - val_loss: 0.1639 - val_accuracy: 0.9518\n",
      "Epoch 15/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2223 - accuracy: 0.9378 - val_loss: 0.1600 - val_accuracy: 0.9525\n",
      "Epoch 16/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2163 - accuracy: 0.9392 - val_loss: 0.1577 - val_accuracy: 0.9531\n",
      "Epoch 17/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2137 - accuracy: 0.9404 - val_loss: 0.1550 - val_accuracy: 0.9533\n",
      "Epoch 18/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2092 - accuracy: 0.9411 - val_loss: 0.1526 - val_accuracy: 0.9536\n",
      "Epoch 19/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2061 - accuracy: 0.9421 - val_loss: 0.1546 - val_accuracy: 0.9534\n",
      "Epoch 20/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.2020 - accuracy: 0.9430 - val_loss: 0.1491 - val_accuracy: 0.9536\n",
      "Epoch 21/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1984 - accuracy: 0.9437 - val_loss: 0.1483 - val_accuracy: 0.9537\n",
      "Epoch 22/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1964 - accuracy: 0.9443 - val_loss: 0.1467 - val_accuracy: 0.9539\n",
      "Epoch 23/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1936 - accuracy: 0.9446 - val_loss: 0.1463 - val_accuracy: 0.9541\n",
      "Epoch 24/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1908 - accuracy: 0.9455 - val_loss: 0.1459 - val_accuracy: 0.9539\n",
      "Epoch 25/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1893 - accuracy: 0.9463 - val_loss: 0.1409 - val_accuracy: 0.9539\n",
      "Epoch 26/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1865 - accuracy: 0.9463 - val_loss: 0.1380 - val_accuracy: 0.9541\n",
      "Epoch 27/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1854 - accuracy: 0.9471 - val_loss: 0.1394 - val_accuracy: 0.9538\n",
      "Epoch 28/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1822 - accuracy: 0.9474 - val_loss: 0.1409 - val_accuracy: 0.9540\n",
      "Epoch 29/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1800 - accuracy: 0.9480 - val_loss: 0.1345 - val_accuracy: 0.9548\n",
      "Epoch 30/300\n",
      "329480/329480 [==============================] - 2s 6us/sample - loss: 0.1792 - accuracy: 0.9482 - val_loss: 0.1334 - val_accuracy: 0.9546\n",
      "Epoch 31/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1776 - accuracy: 0.9485 - val_loss: 0.1327 - val_accuracy: 0.9554\n",
      "Epoch 32/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1757 - accuracy: 0.9493 - val_loss: 0.1320 - val_accuracy: 0.9571\n",
      "Epoch 33/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1733 - accuracy: 0.9496 - val_loss: 0.1322 - val_accuracy: 0.9559\n",
      "Epoch 34/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1724 - accuracy: 0.9497 - val_loss: 0.1311 - val_accuracy: 0.9576\n",
      "Epoch 35/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1715 - accuracy: 0.9502 - val_loss: 0.1351 - val_accuracy: 0.9565\n",
      "Epoch 36/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1695 - accuracy: 0.9501 - val_loss: 0.1267 - val_accuracy: 0.9592\n",
      "Epoch 37/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1685 - accuracy: 0.9508 - val_loss: 0.1301 - val_accuracy: 0.9601\n",
      "Epoch 38/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1666 - accuracy: 0.9510 - val_loss: 0.1269 - val_accuracy: 0.9613\n",
      "Epoch 39/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1661 - accuracy: 0.9509 - val_loss: 0.1263 - val_accuracy: 0.9578\n",
      "Epoch 40/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1650 - accuracy: 0.9516 - val_loss: 0.1266 - val_accuracy: 0.9600\n",
      "Epoch 41/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1632 - accuracy: 0.9517 - val_loss: 0.1260 - val_accuracy: 0.9608\n",
      "Epoch 42/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1618 - accuracy: 0.9520 - val_loss: 0.1246 - val_accuracy: 0.9629\n",
      "Epoch 43/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1594 - accuracy: 0.9522 - val_loss: 0.1203 - val_accuracy: 0.9645\n",
      "Epoch 44/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1599 - accuracy: 0.9524 - val_loss: 0.1186 - val_accuracy: 0.9649\n",
      "Epoch 45/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1579 - accuracy: 0.9525 - val_loss: 0.1220 - val_accuracy: 0.9606\n",
      "Epoch 46/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1562 - accuracy: 0.9532 - val_loss: 0.1140 - val_accuracy: 0.9645\n",
      "Epoch 47/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1541 - accuracy: 0.9535 - val_loss: 0.1138 - val_accuracy: 0.9642\n",
      "Epoch 48/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1532 - accuracy: 0.9537 - val_loss: 0.1125 - val_accuracy: 0.9648\n",
      "Epoch 49/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1514 - accuracy: 0.9541 - val_loss: 0.1154 - val_accuracy: 0.9669\n",
      "Epoch 50/300\n",
      "329480/329480 [==============================] - 2s 5us/sample - loss: 0.1499 - accuracy: 0.9549 - val_loss: 0.1075 - val_accuracy: 0.9666\n",
      "Epoch 51/300\n",
      "329480/329480 [==============================] - 2s 7us/sample - loss: 0.1488 - accuracy: 0.9552 - val_loss: 0.1042 - val_accuracy: 0.9670\n",
      "Epoch 52/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1473 - accuracy: 0.9559 - val_loss: 0.1048 - val_accuracy: 0.9670\n",
      "Epoch 53/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1461 - accuracy: 0.9562 - val_loss: 0.1019 - val_accuracy: 0.9696\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1446 - accuracy: 0.9567 - val_loss: 0.1026 - val_accuracy: 0.9668\n",
      "Epoch 55/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1439 - accuracy: 0.9568 - val_loss: 0.1031 - val_accuracy: 0.9704\n",
      "Epoch 56/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1423 - accuracy: 0.9572 - val_loss: 0.1046 - val_accuracy: 0.9695\n",
      "Epoch 57/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1409 - accuracy: 0.9578 - val_loss: 0.0999 - val_accuracy: 0.9696\n",
      "Epoch 58/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1397 - accuracy: 0.9576 - val_loss: 0.0988 - val_accuracy: 0.9705\n",
      "Epoch 59/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1386 - accuracy: 0.9583 - val_loss: 0.0987 - val_accuracy: 0.9688\n",
      "Epoch 60/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1391 - accuracy: 0.9583 - val_loss: 0.1007 - val_accuracy: 0.9706\n",
      "Epoch 61/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1372 - accuracy: 0.9584 - val_loss: 0.0962 - val_accuracy: 0.9702\n",
      "Epoch 62/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1368 - accuracy: 0.9587 - val_loss: 0.1016 - val_accuracy: 0.9693\n",
      "Epoch 63/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1366 - accuracy: 0.9586 - val_loss: 0.0981 - val_accuracy: 0.9703\n",
      "Epoch 64/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1365 - accuracy: 0.9588 - val_loss: 0.0960 - val_accuracy: 0.9707\n",
      "Epoch 65/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1342 - accuracy: 0.9595 - val_loss: 0.0958 - val_accuracy: 0.9707\n",
      "Epoch 66/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1344 - accuracy: 0.9596 - val_loss: 0.0952 - val_accuracy: 0.9699\n",
      "Epoch 67/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1334 - accuracy: 0.9598 - val_loss: 0.0944 - val_accuracy: 0.9706\n",
      "Epoch 68/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1330 - accuracy: 0.9600 - val_loss: 0.0983 - val_accuracy: 0.9715\n",
      "Epoch 69/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1313 - accuracy: 0.9607 - val_loss: 0.0944 - val_accuracy: 0.9701\n",
      "Epoch 70/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1312 - accuracy: 0.9609 - val_loss: 0.0962 - val_accuracy: 0.9704\n",
      "Epoch 71/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1319 - accuracy: 0.9605 - val_loss: 0.0932 - val_accuracy: 0.9705\n",
      "Epoch 72/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1301 - accuracy: 0.9610 - val_loss: 0.0926 - val_accuracy: 0.9716\n",
      "Epoch 73/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1294 - accuracy: 0.9613 - val_loss: 0.0944 - val_accuracy: 0.9703\n",
      "Epoch 74/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1292 - accuracy: 0.9610 - val_loss: 0.0936 - val_accuracy: 0.9702\n",
      "Epoch 75/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1286 - accuracy: 0.9610 - val_loss: 0.0941 - val_accuracy: 0.9697\n",
      "Epoch 76/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1286 - accuracy: 0.9616 - val_loss: 0.0920 - val_accuracy: 0.9708\n",
      "Epoch 77/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1287 - accuracy: 0.9615 - val_loss: 0.0928 - val_accuracy: 0.9727\n",
      "Epoch 78/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1273 - accuracy: 0.9616 - val_loss: 0.0934 - val_accuracy: 0.9710\n",
      "Epoch 79/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1270 - accuracy: 0.9616 - val_loss: 0.0914 - val_accuracy: 0.9719\n",
      "Epoch 80/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1260 - accuracy: 0.9620 - val_loss: 0.0899 - val_accuracy: 0.9728\n",
      "Epoch 81/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1259 - accuracy: 0.9623 - val_loss: 0.0910 - val_accuracy: 0.9721\n",
      "Epoch 82/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1252 - accuracy: 0.9626 - val_loss: 0.0899 - val_accuracy: 0.9714\n",
      "Epoch 83/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1251 - accuracy: 0.9620 - val_loss: 0.0918 - val_accuracy: 0.9699\n",
      "Epoch 84/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1245 - accuracy: 0.9625 - val_loss: 0.0923 - val_accuracy: 0.9700\n",
      "Epoch 85/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1244 - accuracy: 0.9623 - val_loss: 0.0907 - val_accuracy: 0.9705\n",
      "Epoch 86/300\n",
      "329480/329480 [==============================] - 3s 10us/sample - loss: 0.1237 - accuracy: 0.9630 - val_loss: 0.0896 - val_accuracy: 0.9724\n",
      "Epoch 87/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1231 - accuracy: 0.9629 - val_loss: 0.0893 - val_accuracy: 0.9731\n",
      "Epoch 88/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1240 - accuracy: 0.9625 - val_loss: 0.0893 - val_accuracy: 0.9729\n",
      "Epoch 89/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1226 - accuracy: 0.9632 - val_loss: 0.0885 - val_accuracy: 0.9737\n",
      "Epoch 90/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1223 - accuracy: 0.9632 - val_loss: 0.0891 - val_accuracy: 0.9729\n",
      "Epoch 91/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1219 - accuracy: 0.9637 - val_loss: 0.0911 - val_accuracy: 0.9700\n",
      "Epoch 92/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1217 - accuracy: 0.9632 - val_loss: 0.0893 - val_accuracy: 0.9747\n",
      "Epoch 93/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1210 - accuracy: 0.9635 - val_loss: 0.0912 - val_accuracy: 0.9685\n",
      "Epoch 94/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1209 - accuracy: 0.9638 - val_loss: 0.0878 - val_accuracy: 0.9727\n",
      "Epoch 95/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1207 - accuracy: 0.9641 - val_loss: 0.0889 - val_accuracy: 0.9753\n",
      "Epoch 96/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1198 - accuracy: 0.9639 - val_loss: 0.0921 - val_accuracy: 0.9740\n",
      "Epoch 97/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1196 - accuracy: 0.9639 - val_loss: 0.0882 - val_accuracy: 0.9721\n",
      "Epoch 98/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1207 - accuracy: 0.9640 - val_loss: 0.0883 - val_accuracy: 0.9725\n",
      "Epoch 99/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1199 - accuracy: 0.9637 - val_loss: 0.0879 - val_accuracy: 0.9723\n",
      "Epoch 100/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1190 - accuracy: 0.9643 - val_loss: 0.0908 - val_accuracy: 0.9707\n",
      "Epoch 101/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1189 - accuracy: 0.9641 - val_loss: 0.0907 - val_accuracy: 0.9698\n",
      "Epoch 102/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1187 - accuracy: 0.9642 - val_loss: 0.0863 - val_accuracy: 0.9734\n",
      "Epoch 103/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1180 - accuracy: 0.9647 - val_loss: 0.0899 - val_accuracy: 0.9708\n",
      "Epoch 104/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1174 - accuracy: 0.9647 - val_loss: 0.0878 - val_accuracy: 0.9716\n",
      "Epoch 105/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1174 - accuracy: 0.9648 - val_loss: 0.0864 - val_accuracy: 0.9735\n",
      "Epoch 106/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1179 - accuracy: 0.9646 - val_loss: 0.0890 - val_accuracy: 0.9703\n",
      "Epoch 107/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1166 - accuracy: 0.9649 - val_loss: 0.0870 - val_accuracy: 0.9736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1174 - accuracy: 0.9647 - val_loss: 0.0909 - val_accuracy: 0.9748\n",
      "Epoch 109/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1164 - accuracy: 0.9651 - val_loss: 0.0877 - val_accuracy: 0.9717\n",
      "Epoch 110/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1172 - accuracy: 0.9651 - val_loss: 0.0881 - val_accuracy: 0.9714\n",
      "Epoch 111/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1164 - accuracy: 0.9648 - val_loss: 0.0862 - val_accuracy: 0.9731\n",
      "Epoch 112/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1159 - accuracy: 0.9652 - val_loss: 0.0867 - val_accuracy: 0.9715\n",
      "Epoch 113/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1161 - accuracy: 0.9654 - val_loss: 0.0859 - val_accuracy: 0.9743\n",
      "Epoch 114/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1160 - accuracy: 0.9652 - val_loss: 0.0853 - val_accuracy: 0.9751\n",
      "Epoch 115/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1159 - accuracy: 0.9653 - val_loss: 0.0875 - val_accuracy: 0.9707\n",
      "Epoch 116/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1158 - accuracy: 0.9653 - val_loss: 0.0888 - val_accuracy: 0.9717\n",
      "Epoch 117/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1144 - accuracy: 0.9656 - val_loss: 0.0884 - val_accuracy: 0.9723\n",
      "Epoch 118/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1138 - accuracy: 0.9660 - val_loss: 0.0888 - val_accuracy: 0.9704\n",
      "Epoch 119/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1144 - accuracy: 0.9657 - val_loss: 0.0893 - val_accuracy: 0.9698\n",
      "Epoch 120/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1143 - accuracy: 0.9659 - val_loss: 0.0865 - val_accuracy: 0.9722\n",
      "Epoch 121/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1139 - accuracy: 0.9659 - val_loss: 0.0860 - val_accuracy: 0.9747\n",
      "Epoch 122/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1137 - accuracy: 0.9660 - val_loss: 0.0858 - val_accuracy: 0.9737\n",
      "Epoch 123/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1141 - accuracy: 0.9660 - val_loss: 0.0863 - val_accuracy: 0.9722\n",
      "Epoch 124/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1133 - accuracy: 0.9662 - val_loss: 0.0891 - val_accuracy: 0.9698\n",
      "Epoch 125/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1131 - accuracy: 0.9663 - val_loss: 0.0870 - val_accuracy: 0.9711\n",
      "Epoch 126/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1126 - accuracy: 0.9664 - val_loss: 0.0889 - val_accuracy: 0.9699\n",
      "Epoch 127/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1125 - accuracy: 0.9665 - val_loss: 0.0847 - val_accuracy: 0.9741\n",
      "Epoch 128/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1125 - accuracy: 0.9663 - val_loss: 0.0845 - val_accuracy: 0.9756\n",
      "Epoch 129/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1134 - accuracy: 0.9659 - val_loss: 0.0849 - val_accuracy: 0.9739\n",
      "Epoch 130/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1122 - accuracy: 0.9662 - val_loss: 0.0846 - val_accuracy: 0.9745\n",
      "Epoch 131/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1132 - accuracy: 0.9666 - val_loss: 0.0862 - val_accuracy: 0.9705\n",
      "Epoch 132/300\n",
      "329480/329480 [==============================] - 3s 10us/sample - loss: 0.1119 - accuracy: 0.9668 - val_loss: 0.0829 - val_accuracy: 0.9752\n",
      "Epoch 133/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1113 - accuracy: 0.9669 - val_loss: 0.0827 - val_accuracy: 0.9747\n",
      "Epoch 134/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1110 - accuracy: 0.9668 - val_loss: 0.0855 - val_accuracy: 0.9727\n",
      "Epoch 135/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1116 - accuracy: 0.9669 - val_loss: 0.0859 - val_accuracy: 0.9716\n",
      "Epoch 136/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1119 - accuracy: 0.9666 - val_loss: 0.0855 - val_accuracy: 0.9718\n",
      "Epoch 137/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1110 - accuracy: 0.9669 - val_loss: 0.0869 - val_accuracy: 0.9706\n",
      "Epoch 138/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1112 - accuracy: 0.9667 - val_loss: 0.0843 - val_accuracy: 0.9759\n",
      "Epoch 139/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1116 - accuracy: 0.9668 - val_loss: 0.0868 - val_accuracy: 0.9703\n",
      "Epoch 140/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1102 - accuracy: 0.9669 - val_loss: 0.0888 - val_accuracy: 0.9692\n",
      "Epoch 141/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1116 - accuracy: 0.9671 - val_loss: 0.0855 - val_accuracy: 0.9731\n",
      "Epoch 142/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1102 - accuracy: 0.9674 - val_loss: 0.0840 - val_accuracy: 0.9742\n",
      "Epoch 143/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1108 - accuracy: 0.9671 - val_loss: 0.0880 - val_accuracy: 0.9695\n",
      "Epoch 144/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1102 - accuracy: 0.9671 - val_loss: 0.0822 - val_accuracy: 0.9758\n",
      "Epoch 145/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1091 - accuracy: 0.9676 - val_loss: 0.0830 - val_accuracy: 0.9751\n",
      "Epoch 146/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1101 - accuracy: 0.9673 - val_loss: 0.0828 - val_accuracy: 0.9755\n",
      "Epoch 147/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1102 - accuracy: 0.9672 - val_loss: 0.0838 - val_accuracy: 0.9746\n",
      "Epoch 148/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1102 - accuracy: 0.9674 - val_loss: 0.0829 - val_accuracy: 0.9756\n",
      "Epoch 149/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1090 - accuracy: 0.9674 - val_loss: 0.0832 - val_accuracy: 0.9738\n",
      "Epoch 150/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1092 - accuracy: 0.9676 - val_loss: 0.0818 - val_accuracy: 0.9761\n",
      "Epoch 151/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1085 - accuracy: 0.9676 - val_loss: 0.0822 - val_accuracy: 0.9758\n",
      "Epoch 152/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1091 - accuracy: 0.9675 - val_loss: 0.0835 - val_accuracy: 0.9758\n",
      "Epoch 153/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1088 - accuracy: 0.9676 - val_loss: 0.0862 - val_accuracy: 0.9694\n",
      "Epoch 154/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1089 - accuracy: 0.9675 - val_loss: 0.0829 - val_accuracy: 0.9759\n",
      "Epoch 155/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1091 - accuracy: 0.9673 - val_loss: 0.0829 - val_accuracy: 0.9741\n",
      "Epoch 156/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1079 - accuracy: 0.9677 - val_loss: 0.0829 - val_accuracy: 0.9751\n",
      "Epoch 157/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1085 - accuracy: 0.9678 - val_loss: 0.0823 - val_accuracy: 0.9763\n",
      "Epoch 158/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1089 - accuracy: 0.9674 - val_loss: 0.0837 - val_accuracy: 0.9732\n",
      "Epoch 159/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1091 - accuracy: 0.9676 - val_loss: 0.0822 - val_accuracy: 0.9761\n",
      "Epoch 160/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1077 - accuracy: 0.9678 - val_loss: 0.0826 - val_accuracy: 0.9759\n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1078 - accuracy: 0.9679 - val_loss: 0.0846 - val_accuracy: 0.9729\n",
      "Epoch 162/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1079 - accuracy: 0.9680 - val_loss: 0.0858 - val_accuracy: 0.9714\n",
      "Epoch 163/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1080 - accuracy: 0.9679 - val_loss: 0.0879 - val_accuracy: 0.9708\n",
      "Epoch 164/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1071 - accuracy: 0.9684 - val_loss: 0.0841 - val_accuracy: 0.9719\n",
      "Epoch 165/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1067 - accuracy: 0.9682 - val_loss: 0.0840 - val_accuracy: 0.9719\n",
      "Epoch 166/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1069 - accuracy: 0.9678 - val_loss: 0.0864 - val_accuracy: 0.9712\n",
      "Epoch 167/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1073 - accuracy: 0.9681 - val_loss: 0.0817 - val_accuracy: 0.9764\n",
      "Epoch 168/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1077 - accuracy: 0.9680 - val_loss: 0.0816 - val_accuracy: 0.9753\n",
      "Epoch 169/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1071 - accuracy: 0.9683 - val_loss: 0.0828 - val_accuracy: 0.9737\n",
      "Epoch 170/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1069 - accuracy: 0.9682 - val_loss: 0.0843 - val_accuracy: 0.9718\n",
      "Epoch 171/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1057 - accuracy: 0.9683 - val_loss: 0.0812 - val_accuracy: 0.9761\n",
      "Epoch 172/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1063 - accuracy: 0.9685 - val_loss: 0.0823 - val_accuracy: 0.9744\n",
      "Epoch 173/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1062 - accuracy: 0.9680 - val_loss: 0.0855 - val_accuracy: 0.9709\n",
      "Epoch 174/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1068 - accuracy: 0.9680 - val_loss: 0.0820 - val_accuracy: 0.9756\n",
      "Epoch 175/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1070 - accuracy: 0.9681 - val_loss: 0.0856 - val_accuracy: 0.9713\n",
      "Epoch 176/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1068 - accuracy: 0.9678 - val_loss: 0.0826 - val_accuracy: 0.9761\n",
      "Epoch 177/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1054 - accuracy: 0.9686 - val_loss: 0.0881 - val_accuracy: 0.9702\n",
      "Epoch 178/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1061 - accuracy: 0.9683 - val_loss: 0.0810 - val_accuracy: 0.9750\n",
      "Epoch 179/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1057 - accuracy: 0.9682 - val_loss: 0.0860 - val_accuracy: 0.9712\n",
      "Epoch 180/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1061 - accuracy: 0.9679 - val_loss: 0.0827 - val_accuracy: 0.9748\n",
      "Epoch 181/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1059 - accuracy: 0.9684 - val_loss: 0.0807 - val_accuracy: 0.9764\n",
      "Epoch 182/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1051 - accuracy: 0.9686 - val_loss: 0.0819 - val_accuracy: 0.9755\n",
      "Epoch 183/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1057 - accuracy: 0.9687 - val_loss: 0.0810 - val_accuracy: 0.9756\n",
      "Epoch 184/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1055 - accuracy: 0.9691 - val_loss: 0.0830 - val_accuracy: 0.9759\n",
      "Epoch 185/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1047 - accuracy: 0.9691 - val_loss: 0.0803 - val_accuracy: 0.9765\n",
      "Epoch 186/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1050 - accuracy: 0.9691 - val_loss: 0.0802 - val_accuracy: 0.9761\n",
      "Epoch 187/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1046 - accuracy: 0.9687 - val_loss: 0.0823 - val_accuracy: 0.9736\n",
      "Epoch 188/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1057 - accuracy: 0.9684 - val_loss: 0.0818 - val_accuracy: 0.9757\n",
      "Epoch 189/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1053 - accuracy: 0.9688 - val_loss: 0.0844 - val_accuracy: 0.9721\n",
      "Epoch 190/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1051 - accuracy: 0.9688 - val_loss: 0.0802 - val_accuracy: 0.9765\n",
      "Epoch 191/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1049 - accuracy: 0.9689 - val_loss: 0.0811 - val_accuracy: 0.9760\n",
      "Epoch 192/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1046 - accuracy: 0.9687 - val_loss: 0.0829 - val_accuracy: 0.9730\n",
      "Epoch 193/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1048 - accuracy: 0.9686 - val_loss: 0.0847 - val_accuracy: 0.9710\n",
      "Epoch 194/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1040 - accuracy: 0.9691 - val_loss: 0.0803 - val_accuracy: 0.9763\n",
      "Epoch 195/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.1041 - accuracy: 0.9692 - val_loss: 0.0818 - val_accuracy: 0.9734\n",
      "Epoch 196/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1037 - accuracy: 0.9693 - val_loss: 0.0817 - val_accuracy: 0.9718\n",
      "Epoch 197/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1045 - accuracy: 0.9690 - val_loss: 0.0816 - val_accuracy: 0.9758\n",
      "Epoch 198/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1038 - accuracy: 0.9689 - val_loss: 0.0805 - val_accuracy: 0.9764\n",
      "Epoch 199/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1040 - accuracy: 0.9691 - val_loss: 0.0813 - val_accuracy: 0.9765\n",
      "Epoch 200/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1027 - accuracy: 0.9693 - val_loss: 0.0804 - val_accuracy: 0.9753\n",
      "Epoch 201/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1038 - accuracy: 0.9690 - val_loss: 0.0821 - val_accuracy: 0.9761\n",
      "Epoch 202/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1031 - accuracy: 0.9693 - val_loss: 0.0834 - val_accuracy: 0.9721\n",
      "Epoch 203/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1042 - accuracy: 0.9692 - val_loss: 0.0813 - val_accuracy: 0.9738\n",
      "Epoch 204/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1034 - accuracy: 0.9693 - val_loss: 0.0807 - val_accuracy: 0.9770\n",
      "Epoch 205/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1035 - accuracy: 0.9694 - val_loss: 0.0805 - val_accuracy: 0.9765\n",
      "Epoch 206/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1034 - accuracy: 0.9694 - val_loss: 0.0830 - val_accuracy: 0.9720\n",
      "Epoch 207/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1035 - accuracy: 0.9691 - val_loss: 0.0850 - val_accuracy: 0.9714\n",
      "Epoch 208/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1028 - accuracy: 0.9694 - val_loss: 0.0792 - val_accuracy: 0.9758\n",
      "Epoch 209/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1043 - accuracy: 0.9693 - val_loss: 0.0816 - val_accuracy: 0.9724\n",
      "Epoch 210/300\n",
      "327680/329480 [============================>.] - ETA: 0s - loss: 0.1027 - accuracy: 0.9695\n",
      "Epoch 00210: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1029 - accuracy: 0.9695 - val_loss: 0.0799 - val_accuracy: 0.9761\n",
      "Epoch 211/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1020 - accuracy: 0.9698 - val_loss: 0.0802 - val_accuracy: 0.9763\n",
      "Epoch 212/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1015 - accuracy: 0.9697 - val_loss: 0.0797 - val_accuracy: 0.9749\n",
      "Epoch 213/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1003 - accuracy: 0.9706 - val_loss: 0.0781 - val_accuracy: 0.9769\n",
      "Epoch 214/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0998 - accuracy: 0.9707 - val_loss: 0.0775 - val_accuracy: 0.9768\n",
      "Epoch 215/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1004 - accuracy: 0.9705 - val_loss: 0.0826 - val_accuracy: 0.9725\n",
      "Epoch 216/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1004 - accuracy: 0.9704 - val_loss: 0.0783 - val_accuracy: 0.9767\n",
      "Epoch 217/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0999 - accuracy: 0.9708 - val_loss: 0.0777 - val_accuracy: 0.9765\n",
      "Epoch 218/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1011 - accuracy: 0.9704 - val_loss: 0.0775 - val_accuracy: 0.9772\n",
      "Epoch 219/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0998 - accuracy: 0.9705 - val_loss: 0.0817 - val_accuracy: 0.9718\n",
      "Epoch 220/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1000 - accuracy: 0.9708 - val_loss: 0.0775 - val_accuracy: 0.9768\n",
      "Epoch 221/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1001 - accuracy: 0.9706 - val_loss: 0.0799 - val_accuracy: 0.9753\n",
      "Epoch 222/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1000 - accuracy: 0.9707 - val_loss: 0.0783 - val_accuracy: 0.9772\n",
      "Epoch 223/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1000 - accuracy: 0.9706 - val_loss: 0.0859 - val_accuracy: 0.9713\n",
      "Epoch 224/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0994 - accuracy: 0.9711 - val_loss: 0.0783 - val_accuracy: 0.9765\n",
      "Epoch 225/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1000 - accuracy: 0.9707 - val_loss: 0.0852 - val_accuracy: 0.9713\n",
      "Epoch 226/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0999 - accuracy: 0.9710 - val_loss: 0.0787 - val_accuracy: 0.9763\n",
      "Epoch 227/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0996 - accuracy: 0.9708 - val_loss: 0.0789 - val_accuracy: 0.9761\n",
      "Epoch 228/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0991 - accuracy: 0.9711 - val_loss: 0.0777 - val_accuracy: 0.9768\n",
      "Epoch 229/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0998 - accuracy: 0.9708 - val_loss: 0.0792 - val_accuracy: 0.9754\n",
      "Epoch 230/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0996 - accuracy: 0.9709 - val_loss: 0.0771 - val_accuracy: 0.9769\n",
      "Epoch 231/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0992 - accuracy: 0.9711 - val_loss: 0.0791 - val_accuracy: 0.9768\n",
      "Epoch 232/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0986 - accuracy: 0.9708 - val_loss: 0.0785 - val_accuracy: 0.9760\n",
      "Epoch 233/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0994 - accuracy: 0.9711 - val_loss: 0.0795 - val_accuracy: 0.9750\n",
      "Epoch 234/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0986 - accuracy: 0.9712 - val_loss: 0.0840 - val_accuracy: 0.9701\n",
      "Epoch 235/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0990 - accuracy: 0.9708 - val_loss: 0.0771 - val_accuracy: 0.9769\n",
      "Epoch 236/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0992 - accuracy: 0.9710 - val_loss: 0.0776 - val_accuracy: 0.9766\n",
      "Epoch 237/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.1005 - accuracy: 0.9707 - val_loss: 0.0766 - val_accuracy: 0.9770\n",
      "Epoch 238/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0996 - accuracy: 0.9709 - val_loss: 0.0779 - val_accuracy: 0.9768\n",
      "Epoch 239/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0984 - accuracy: 0.9714 - val_loss: 0.0782 - val_accuracy: 0.9762\n",
      "Epoch 240/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0989 - accuracy: 0.9711 - val_loss: 0.0774 - val_accuracy: 0.9770\n",
      "Epoch 241/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0992 - accuracy: 0.9713 - val_loss: 0.0817 - val_accuracy: 0.9721\n",
      "Epoch 242/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0992 - accuracy: 0.9709 - val_loss: 0.0777 - val_accuracy: 0.9771\n",
      "Epoch 243/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0983 - accuracy: 0.9713 - val_loss: 0.0778 - val_accuracy: 0.9770\n",
      "Epoch 244/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0985 - accuracy: 0.9714 - val_loss: 0.0794 - val_accuracy: 0.9765\n",
      "Epoch 245/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0987 - accuracy: 0.9712 - val_loss: 0.0799 - val_accuracy: 0.9761\n",
      "Epoch 246/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0984 - accuracy: 0.9715 - val_loss: 0.0779 - val_accuracy: 0.9772\n",
      "Epoch 247/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0984 - accuracy: 0.9714 - val_loss: 0.0785 - val_accuracy: 0.9743\n",
      "Epoch 248/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0976 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9771\n",
      "Epoch 249/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0985 - accuracy: 0.9711 - val_loss: 0.0857 - val_accuracy: 0.9711\n",
      "Epoch 250/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0981 - accuracy: 0.9714 - val_loss: 0.0769 - val_accuracy: 0.9771\n",
      "Epoch 251/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0975 - accuracy: 0.9717 - val_loss: 0.0773 - val_accuracy: 0.9766\n",
      "Epoch 252/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0976 - accuracy: 0.9715 - val_loss: 0.0773 - val_accuracy: 0.9768\n",
      "Epoch 253/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0988 - accuracy: 0.9713 - val_loss: 0.0769 - val_accuracy: 0.9768\n",
      "Epoch 254/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0983 - accuracy: 0.9712 - val_loss: 0.0770 - val_accuracy: 0.9771\n",
      "Epoch 255/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0977 - accuracy: 0.9714 - val_loss: 0.0796 - val_accuracy: 0.9760\n",
      "Epoch 256/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0987 - accuracy: 0.9710 - val_loss: 0.0799 - val_accuracy: 0.9754\n",
      "Epoch 257/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0980 - accuracy: 0.9715 - val_loss: 0.0794 - val_accuracy: 0.9768\n",
      "Epoch 258/300\n",
      "322560/329480 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9712\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0985 - accuracy: 0.9712 - val_loss: 0.0791 - val_accuracy: 0.9770\n",
      "Epoch 259/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0977 - accuracy: 0.9718 - val_loss: 0.0780 - val_accuracy: 0.9770\n",
      "Epoch 260/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0966 - accuracy: 0.9722 - val_loss: 0.0764 - val_accuracy: 0.9772\n",
      "Epoch 261/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0967 - accuracy: 0.9721 - val_loss: 0.0767 - val_accuracy: 0.9770\n",
      "Epoch 262/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0965 - accuracy: 0.9719 - val_loss: 0.0771 - val_accuracy: 0.9769\n",
      "Epoch 263/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0972 - accuracy: 0.9720 - val_loss: 0.0781 - val_accuracy: 0.9772\n",
      "Epoch 264/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0967 - accuracy: 0.9720 - val_loss: 0.0763 - val_accuracy: 0.9771\n",
      "Epoch 265/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0975 - accuracy: 0.9719 - val_loss: 0.0775 - val_accuracy: 0.9769\n",
      "Epoch 266/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0970 - accuracy: 0.9721 - val_loss: 0.0758 - val_accuracy: 0.9769\n",
      "Epoch 267/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0964 - accuracy: 0.9721 - val_loss: 0.0761 - val_accuracy: 0.9771\n",
      "Epoch 268/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0962 - accuracy: 0.9723 - val_loss: 0.0770 - val_accuracy: 0.9766\n",
      "Epoch 269/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0963 - accuracy: 0.9723 - val_loss: 0.0760 - val_accuracy: 0.9771\n",
      "Epoch 270/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0964 - accuracy: 0.9720 - val_loss: 0.0772 - val_accuracy: 0.9769\n",
      "Epoch 271/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0968 - accuracy: 0.9719 - val_loss: 0.0773 - val_accuracy: 0.9771\n",
      "Epoch 272/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0958 - accuracy: 0.9723 - val_loss: 0.0764 - val_accuracy: 0.9771\n",
      "Epoch 273/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0960 - accuracy: 0.9719 - val_loss: 0.0762 - val_accuracy: 0.9770\n",
      "Epoch 274/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0955 - accuracy: 0.9722 - val_loss: 0.0767 - val_accuracy: 0.9766\n",
      "Epoch 275/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0955 - accuracy: 0.9722 - val_loss: 0.0776 - val_accuracy: 0.9761\n",
      "Epoch 276/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0955 - accuracy: 0.9724 - val_loss: 0.0772 - val_accuracy: 0.9772\n",
      "Epoch 277/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0955 - accuracy: 0.9723 - val_loss: 0.0763 - val_accuracy: 0.9769\n",
      "Epoch 278/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0957 - accuracy: 0.9721 - val_loss: 0.0771 - val_accuracy: 0.9768\n",
      "Epoch 279/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0960 - accuracy: 0.9720 - val_loss: 0.0760 - val_accuracy: 0.9769\n",
      "Epoch 280/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0961 - accuracy: 0.9723 - val_loss: 0.0763 - val_accuracy: 0.9770\n",
      "Epoch 281/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0963 - accuracy: 0.9720 - val_loss: 0.0760 - val_accuracy: 0.9770\n",
      "Epoch 282/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0958 - accuracy: 0.9721 - val_loss: 0.0779 - val_accuracy: 0.9770\n",
      "Epoch 283/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0958 - accuracy: 0.9723 - val_loss: 0.0772 - val_accuracy: 0.9770\n",
      "Epoch 284/300\n",
      "325120/329480 [============================>.] - ETA: 0s - loss: 0.0957 - accuracy: 0.9725\n",
      "Epoch 00284: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0957 - accuracy: 0.9724 - val_loss: 0.0776 - val_accuracy: 0.9772\n",
      "Epoch 285/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0959 - accuracy: 0.9722 - val_loss: 0.0759 - val_accuracy: 0.9770\n",
      "Epoch 286/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0960 - accuracy: 0.9725 - val_loss: 0.0755 - val_accuracy: 0.9772\n",
      "Epoch 287/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0957 - accuracy: 0.9726 - val_loss: 0.0773 - val_accuracy: 0.9768\n",
      "Epoch 288/300\n",
      "329480/329480 [==============================] - 3s 8us/sample - loss: 0.0951 - accuracy: 0.9725 - val_loss: 0.0761 - val_accuracy: 0.9771\n",
      "Epoch 289/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0954 - accuracy: 0.9726 - val_loss: 0.0762 - val_accuracy: 0.9770\n",
      "Epoch 290/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0949 - accuracy: 0.9726 - val_loss: 0.0762 - val_accuracy: 0.9769\n",
      "Epoch 291/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0944 - accuracy: 0.9727 - val_loss: 0.0756 - val_accuracy: 0.9770\n",
      "Epoch 292/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0953 - accuracy: 0.9724 - val_loss: 0.0772 - val_accuracy: 0.9773\n",
      "Epoch 293/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0950 - accuracy: 0.9724 - val_loss: 0.0761 - val_accuracy: 0.9771\n",
      "Epoch 294/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0946 - accuracy: 0.9727 - val_loss: 0.0761 - val_accuracy: 0.9769\n",
      "Epoch 295/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0944 - accuracy: 0.9726 - val_loss: 0.0753 - val_accuracy: 0.9770\n",
      "Epoch 296/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0950 - accuracy: 0.9728 - val_loss: 0.0761 - val_accuracy: 0.9769\n",
      "Epoch 297/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0944 - accuracy: 0.9727 - val_loss: 0.0764 - val_accuracy: 0.9771\n",
      "Epoch 298/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0945 - accuracy: 0.9724 - val_loss: 0.0762 - val_accuracy: 0.9772\n",
      "Epoch 299/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0948 - accuracy: 0.9726 - val_loss: 0.0757 - val_accuracy: 0.9768\n",
      "Epoch 300/300\n",
      "329480/329480 [==============================] - 3s 9us/sample - loss: 0.0945 - accuracy: 0.9726 - val_loss: 0.0752 - val_accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "monitor = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor=0.5,mode=\"min\",patience=10,verbose=1,min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('Best_Model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=300, batch_size=512, callbacks=[monitor, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9763719303996286\n"
     ]
    }
   ],
   "source": [
    "# Load the best-saved model\n",
    "best_model = load_model('Best_Model.h5')\n",
    "\n",
    "# Evaluate the best-saved model\n",
    "y_pred = best_model.predict(x_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "acc = accuracy_score(y_val, y_pred_classes)\n",
    "print('')\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "ax.plot(history.history['loss'], label='train')\n",
    "ax.plot(history.history['val_loss'], label='test')\n",
    "ax.set_title('Model Loss')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hVv8L2EN_XN",
    "outputId": "6d8a189e-7051-42a6-dd0d-b626eb55d212"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zjwPGzH7cXls"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def calculate_metrics(modelName, yTrue, yPred, average='binary'):\n",
    "    \"\"\"\n",
    "    Calculate and print the performance metrics of a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    modelName (str): The name of the classification model.\n",
    "    yTrue (array-like): The true labels.\n",
    "    yPred (array-like): The predicted labels.\n",
    "    average (str or None, optional): The averaging method to use for multi-class classification. One of \n",
    "        {'micro', 'macro', 'weighted', 'binary'} or None (default: 'binary'). If None, only binary \n",
    "        classification metrics will be computed.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If `average` is not one of {'micro', 'macro', 'weighted', 'binary'} or None.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check if average parameter is valid\n",
    "    if average != 'micro' and average != 'macro' and average != 'weighted' and average != 'binary' and average != None:\n",
    "        print(\"Average must be one of this options: {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None, default=’binary’\")\n",
    "        return\n",
    "    \n",
    "    # Prints the name of the model and calculate accuracy and precision\n",
    "    print(f\"--- Performance of {modelName} ---\")\n",
    "    acc = accuracy_score(y_true = yTrue, y_pred = yPred)\n",
    "    precision = precision_score(y_true = yTrue, y_pred = yPred, average = average)\n",
    "    print(f'Accuracy : {np.round(acc*100,2)}%\\nPrecision: {np.round(precision*100,2)}%')\n",
    "    \n",
    "    # Calculates and print recall and F1-score\n",
    "    f1 = f1_score(y_true = yTrue, y_pred = yPred, average = average)\n",
    "    recall = recall_score(y_true = yTrue, y_pred = yPred, average = average)\n",
    "    print(f'Recall: {np.round(recall*100,2)}%\\nF1-score: {np.round(f1*100,2)}%')\n",
    "    \n",
    "    #auc_sklearn = roc_auc_score(y_true = yTrue, y_score = yPred, average = average)\n",
    "    #print(f'Roc auc: {np.round(auc_sklearn*100,2)}%')\n",
    "    \n",
    "    # Calculates and prints balanced accuracy and classification report\n",
    "    print(f\"Balanced accuracy: {np.round(balanced_accuracy_score(yTrue, yPred)*100,2)}%\")\n",
    "    print(f\"Classification report:\\n{classification_report(yTrue, yPred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7opqgASr6fxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performance of Neural Network ---\n",
      "Accuracy : 97.64%\n",
      "Precision: 97.8%\n",
      "Recall: 97.64%\n",
      "F1-score: 97.63%\n",
      "Balanced accuracy: 96.66%\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       578\n",
      "           1       0.98      0.99      0.98      2861\n",
      "           2       1.00      1.00      1.00     15566\n",
      "           3       1.00      0.98      0.99      1252\n",
      "           4       1.00      1.00      1.00     15733\n",
      "           5       0.99      1.00      1.00     75000\n",
      "           6       0.98      0.85      0.91     17902\n",
      "           7       0.86      0.97      0.91     15811\n",
      "           8       0.97      0.95      0.96      1733\n",
      "\n",
      "    accuracy                           0.98    146436\n",
      "   macro avg       0.97      0.97      0.97    146436\n",
      "weighted avg       0.98      0.98      0.98    146436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "calculate_metrics(\"Neural Network\", y_val, pred_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random state definition\n",
    "random_state=42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Classification Metrics: Accuracy, Recall, Precision & F1Score\n",
    "def compute_metrics(y_test, pred):\n",
    "    print('Accuracy:', metrics.accuracy_score(y_test, pred))\n",
    "    print('Recall:', metrics.recall_score(y_test, pred, average = 'weighted'))\n",
    "    print('Precision:', metrics.precision_score(y_test, pred, average = 'weighted'))\n",
    "    print('F1-Score:', metrics.f1_score(y_test, pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extra_tree_classifier(x_train, y_train, x_columns):\n",
    "    # Usage of ExtraTreesClassifier for feature selection\n",
    "    extra_tree_forest = ExtraTreesClassifier(n_estimators=5, criterion='entropy', max_features=2, random_state=random_state)\n",
    "    extra_tree_forest.fit(x_train, y_train)\n",
    "    feature_importances = extra_tree_forest.feature_importances_\n",
    "    feature_importance_normalized = np.std([tree.feature_importances_ for tree in  extra_tree_forest.estimators_], axis = 0)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.rcParams[\"figure.figsize\"] = (70, 40)\n",
    "    plt.bar(x_columns, feature_importance_normalized, align='center')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.title('Comparison of different feature importances in the current dataset')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ATTACKS = [\n",
    "    'Apache Killer', 'ARP Spoofing', \n",
    "    'CAM Overflow', 'MQTT Malaria', \n",
    "    'Netscan', 'Normal', \n",
    "    'RUDY', 'SlowLoris', \n",
    "    'SlowRead'\n",
    "]\n",
    "    \n",
    "def conf_matrix(y_test, prediction, array_dimension = 2):\n",
    "    if (array_dimension == 2):\n",
    "        predict_classes = np.argmax(prediction, axis = 1)\n",
    "        expected_classes = np.argmax(y_test, axis = 1)\n",
    "    elif (array_dimension == 1):\n",
    "        predict_classes = prediction\n",
    "        expected_classes = y_test\n",
    "    \n",
    "    cm = confusion_matrix(expected_classes, predict_classes)\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=ATTACKS)\n",
    "\n",
    "    # Plot size\n",
    "    fig, ax = plt.subplots(figsize=(11,11))\n",
    "    \n",
    "    cmd.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "    \n",
    "    # Add axis labels and rotate them\n",
    "    ax.set_xlabel('Predicted labels', rotation=0, labelpad=20, fontsize=11)\n",
    "    ax.set_ylabel('True labels', rotation=90, labelpad=20, fontsize=11)\n",
    "\n",
    "    ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    ax.set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    ax.set_xticklabels(ATTACKS)\n",
    "    ax.set_yticklabels(ATTACKS)\n",
    "        \n",
    "    ax.tick_params(axis='x', pad=35)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha='center', va='center')\n",
    "    ax.tick_params(axis='y', pad=35)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=40, ha='center', va='center')\n",
    "    \n",
    "    # Adjust colorbar size\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=11)  # Adjust the font size of colorbar labels\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.03,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(cmd.im_, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshowac(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=90)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRICS] Decision Tree Classifier\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-22e55485e7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[METRICS] Decision Tree Classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n[CMATRIX] Decision Tree Confusion Matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f4eeae54c418>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(y_test, pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Classification Metrics: Accuracy, Recall, Precision & F1Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 93\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "print(f'[METRICS] Decision Tree Classifier')\n",
    "compute_metrics(y_val, pred)\n",
    "\n",
    "print(f'\\n[CMATRIX] Decision Tree Confusion Matrix')\n",
    "conf_matrix(y_val, pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
