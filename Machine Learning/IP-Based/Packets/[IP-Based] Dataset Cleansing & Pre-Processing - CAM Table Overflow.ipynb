{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c89308e",
   "metadata": {},
   "source": [
    "### Declaration of Functions\n",
    "\n",
    "The section below presents all the functions used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb07139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Read the dataset and return a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(dataset):\n",
    "    df = pd.read_csv(dataset, nrows=380000)\n",
    "    print('[DONE] Dataset imported successfully')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e998d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Delete unnecessary fields\n",
    "def delete_fields(df, fields):\n",
    "    print('[INFO] Deleting unnecessary fields')\n",
    "    for i in fields:\n",
    "        df.drop(i, axis=1, inplace=True)\n",
    "        print(f'[REMOVED] ' + i)\n",
    "    print('[INFO] Unnecessary fields deleted with success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Z-Score normalization\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def zscore_normalization(df, fields):\n",
    "    for i in fields:\n",
    "        df[i] = zscore(df[i])\n",
    "    print(f'[DONE] Z-Score Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464a2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Fill empty cells with N value\n",
    "def fill_empty_cells(df, fields, n):\n",
    "    for i in fields:\n",
    "        df[i] = df[i].fillna(n)\n",
    "    print(f'[DONE] Empty Cells Filling: ' + n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d52d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert fields to dummy variables\n",
    "def dummy_encode(df, fields):\n",
    "    for i in fields:\n",
    "        df = pd.concat([df, pd.get_dummies(df[i], prefix=i, dtype=float)], axis=1)\n",
    "        df.drop(i, axis=1, inplace=True)\n",
    "    print(f'[DONE] Categorical Values (Dummies)')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ca47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert hexadecimal to int\n",
    "def hexadecimal_convert(df, fields):\n",
    "    for i in fields:\n",
    "        df[i] = df[i].astype(str)\n",
    "        df[i] = df[i].apply(int, base=16)\n",
    "    print(f'[DONE] Integer Convert (Hex - Int)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f83ba",
   "metadata": {},
   "source": [
    "### Declaration of Global Variables\n",
    "\n",
    "The section below presents all the global functions used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ed5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '../../../Datasets/IP-Based/Samples/CAM.csv'\n",
    "DATASET_PRE_PROCESSING = '../../../Datasets/IP-Based/CAM.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577a44d",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "The upcoming step will load the desired dataset and read all the data from it, storing it in the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f25f3f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import dataset and read the data into a dataframe\n",
    "df = read_data(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31c433",
   "metadata": {},
   "source": [
    "### Labelling the Dataset\n",
    "\n",
    "This next section will cover the labelling process of the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a337430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Protocols mask applied on the dataframe\n"
     ]
    }
   ],
   "source": [
    "# Creates a mask to only contain the malicious IP address\n",
    "mask = (df['frame.protocols'] == 'eth:ethertype:ip')\n",
    "\n",
    "# Filter the DataFrame to keep only the rows matching the IP address\n",
    "df = df[mask]\n",
    "\n",
    "print(f'[DONE] Protocols mask applied on the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06870175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset malicious labeling done with success\n"
     ]
    }
   ],
   "source": [
    "# Labelling the malicious packets\n",
    "import numpy as np\n",
    "\n",
    "df['is_malicious'] = 1\n",
    "df['attack_type'] = 6\n",
    "print(f'[DONE] Dataset malicious labeling done with success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c916d3",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "\n",
    "This section covers all the data cleaning done in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d763dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleting unnecessary fields\n",
      "[REMOVED] frame.encap_type\n",
      "[REMOVED] frame.ignored\n",
      "[REMOVED] frame.interface_id\n",
      "[REMOVED] frame.interface_name\n",
      "[REMOVED] frame.marked\n",
      "[REMOVED] frame.number\n",
      "[REMOVED] frame.offset_shift\n",
      "[REMOVED] frame.protocols\n",
      "[REMOVED] frame.time\n",
      "[REMOVED] frame.time_delta\n",
      "[REMOVED] frame.time_delta_displayed\n",
      "[REMOVED] frame.time_epoch\n",
      "[REMOVED] frame.time_relative\n",
      "[REMOVED] ip.addr\n",
      "[REMOVED] ip.checksum\n",
      "[REMOVED] ip.checksum.status\n",
      "[REMOVED] ip.dsfield\n",
      "[REMOVED] ip.dsfield.dscp\n",
      "[REMOVED] ip.dsfield.ecn\n",
      "[REMOVED] ip.dst\n",
      "[REMOVED] ip.dst_host\n",
      "[REMOVED] ip.hdr_len\n",
      "[REMOVED] ip.host\n",
      "[REMOVED] ip.id\n",
      "[REMOVED] ip.len\n",
      "[REMOVED] ip.proto\n",
      "[REMOVED] ip.src\n",
      "[REMOVED] ip.src_host\n",
      "[REMOVED] ip.ttl.lncb\n",
      "[REMOVED] eth.padding\n",
      "[REMOVED] ip.flags\n",
      "[REMOVED] ip.ttl\n",
      "[REMOVED] eth.addr\n",
      "[REMOVED] eth.addr.oui\n",
      "[REMOVED] eth.addr.oui_resolved\n",
      "[REMOVED] eth.addr_resolved\n",
      "[REMOVED] eth.dst\n",
      "[REMOVED] eth.dst.lg\n",
      "[REMOVED] eth.dst.oui\n",
      "[REMOVED] eth.dst.oui_resolved\n",
      "[REMOVED] eth.src\n",
      "[REMOVED] eth.src.oui\n",
      "[REMOVED] eth.src.oui_resolved\n",
      "[REMOVED] eth.src_resolved\n",
      "[REMOVED] eth.trailer\n",
      "[REMOVED] eth.type\n",
      "[REMOVED] ip.flags.df\n",
      "[REMOVED] ip.flags.mf\n",
      "[REMOVED] ip.flags.rb\n",
      "[REMOVED] ip.frag_offset\n",
      "[REMOVED] eth.dst_resolved\n",
      "[INFO] Unnecessary fields deleted with success\n"
     ]
    }
   ],
   "source": [
    "# Array: Fields to delete (IDs, Timestamps, Checksums, etc...)\n",
    "fieldsToDelete = [\n",
    "    'frame.encap_type', 'frame.ignored', 'frame.interface_id', 'frame.interface_name', 'frame.marked', \n",
    "    'frame.number', 'frame.offset_shift', 'frame.protocols', 'frame.time', 'frame.time_delta',\n",
    "    'frame.time_delta_displayed', 'frame.time_epoch', 'frame.time_relative', 'ip.addr', 'ip.checksum',\n",
    "    'ip.checksum.status', 'ip.dsfield', 'ip.dsfield.dscp', 'ip.dsfield.ecn', 'ip.dst', 'ip.dst_host', 'ip.hdr_len',\n",
    "    'ip.host', 'ip.id', 'ip.len', 'ip.proto', 'ip.src', 'ip.src_host', 'ip.ttl.lncb', 'eth.padding', 'ip.flags',\n",
    "    'ip.ttl', 'eth.addr', 'eth.addr.oui', 'eth.addr.oui_resolved', 'eth.addr_resolved', 'eth.dst', 'eth.dst.lg',\n",
    "    'eth.dst.oui', 'eth.dst.oui_resolved', 'eth.src', 'eth.src.oui', 'eth.src.oui_resolved', 'eth.src_resolved',\n",
    "    'eth.trailer', 'eth.type', 'ip.flags.df', 'ip.flags.mf', 'ip.flags.rb', 'ip.frag_offset', 'eth.dst_resolved'\n",
    "]\n",
    "\n",
    "delete_fields(df, fieldsToDelete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d887aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eth.fcs',\n",
       " 'eth.fcs.status',\n",
       " 'eth.invalid_lentype',\n",
       " 'eth.invalid_lentype.expert',\n",
       " 'eth.len',\n",
       " 'eth.len.past_end',\n",
       " 'eth.padding_bad']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all empty collumns\n",
    "empty_columns = []\n",
    "for column in df.columns:\n",
    "    if df[column].isnull().all():\n",
    "        empty_columns.append(column)\n",
    "list(empty_columns)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25371d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleting empty columns\n",
      "[REMOVED] eth.fcs\n",
      "[REMOVED] eth.fcs.status\n",
      "[REMOVED] eth.invalid_lentype\n",
      "[REMOVED] eth.invalid_lentype.expert\n",
      "[REMOVED] eth.len\n",
      "[REMOVED] eth.len.past_end\n",
      "[REMOVED] eth.padding_bad\n",
      "[INFO] Empty columns deleted with success\n"
     ]
    }
   ],
   "source": [
    "# Delete and print the empty columns\n",
    "print('[INFO] Deleting empty columns')\n",
    "for column in empty_columns:\n",
    "    df.drop(column, axis = 1, inplace = True)\n",
    "    print(f'[REMOVED] ' + column)\n",
    "print('[INFO] Empty columns deleted with success')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d9d7e",
   "metadata": {},
   "source": [
    "### Pre-Processing & Data Encoding\n",
    "\n",
    "This section covers all the pre-processing and data encoding done in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfad27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Empty Cells Filling: 0\n"
     ]
    }
   ],
   "source": [
    "# Array: Fields to fill with 0\n",
    "fill_0 = [\n",
    "    'eth.src_not_group'\n",
    "]\n",
    "\n",
    "fill_empty_cells(df, fill_0, '0') # Fill: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0e2139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] tcp.option_len: values 10 and 4 were replaced.\n"
     ]
    }
   ],
   "source": [
    "# Value replacing\n",
    "df['frame.cap_len'] = df['frame.cap_len'].replace(54, 0)\n",
    "df['frame.len'] = df['frame.len'].replace(54, 0)\n",
    "print(f'[DONE] tcp.option_len: values 10 and 4 were replaced.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1892449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Convertion to float done with success\n"
     ]
    }
   ],
   "source": [
    "# Converting from dtypes to float\n",
    "for i in df:\n",
    "    df[i] = df[i].astype(float)\n",
    "print(f'[DONE] Convertion to float done with success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4bcaed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset with pre-processing saved with success\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with all the pre-processing and data encoding done\n",
    "df.to_csv(DATASET_PRE_PROCESSING, index=False)\n",
    "print(f'[DONE] Dataset with pre-processing saved with success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
