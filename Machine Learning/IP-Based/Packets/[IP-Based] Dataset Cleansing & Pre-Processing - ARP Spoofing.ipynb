{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c89308e",
   "metadata": {},
   "source": [
    "### Declaration of Functions\n",
    "\n",
    "The section below presents all the functions used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb07139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Read the dataset and return a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(dataset):\n",
    "    df = pd.read_csv(dataset, nrows=380000)\n",
    "    print('[DONE] Dataset imported successfully')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e998d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Delete unnecessary fields\n",
    "def delete_fields(df, fields):\n",
    "    print('[INFO] Deleting unnecessary fields')\n",
    "    for i in fields:\n",
    "        df.drop(i, axis=1, inplace=True)\n",
    "        print(f'[REMOVED] ' + i)\n",
    "    print('[INFO] Unnecessary fields deleted with success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Z-Score normalization\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def zscore_normalization(df, fields):\n",
    "    for i in fields:\n",
    "        df[i] = zscore(df[i])\n",
    "    print(f'[DONE] Z-Score Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464a2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Fill empty cells with N value\n",
    "def fill_empty_cells(df, fields, n):\n",
    "    for i in fields:\n",
    "        df[i] = df[i].fillna(n)\n",
    "    print(f'[DONE] Empty Cells Filling: ' + n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d52d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert fields to dummy variables\n",
    "def dummy_encode(df, fields):\n",
    "    for i in fields:\n",
    "        df = pd.concat([df, pd.get_dummies(df[i], prefix=i, dtype=float)], axis=1)\n",
    "        df.drop(i, axis=1, inplace=True)\n",
    "    print(f'[DONE] Categorical Values (Dummies)')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ca47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert hexadecimal to int\n",
    "def hexadecimal_convert(df, fields):\n",
    "    for i in fields:\n",
    "        df[i] = df[i].astype(str)\n",
    "        df[i] = df[i].apply(int, base=16)\n",
    "    print(f'[DONE] Integer Convert (Hex - Int)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f83ba",
   "metadata": {},
   "source": [
    "### Declaration of Global Variables\n",
    "\n",
    "The section below presents all the global functions used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ed5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '../../../Datasets/IP-Based/Samples/ARP.csv'\n",
    "DATASET_PRE_PROCESSING = '../../../Datasets/IP-Based/ARP.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577a44d",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "The upcoming step will load the desired dataset and read all the data from it, storing it in the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f25f3f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import dataset and read the data into a dataframe\n",
    "df = read_data(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31c433",
   "metadata": {},
   "source": [
    "### Labelling the Dataset\n",
    "\n",
    "This next section will cover the labelling process of the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a337430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Protocols mask applied on the dataframe\n"
     ]
    }
   ],
   "source": [
    "# Creates a mask to only contain the malicious IP address\n",
    "mask = (df['frame.protocols'] == 'sll:ethertype:arp')\n",
    "\n",
    "# Filter the DataFrame to keep only the rows matching the IP address\n",
    "df = df[mask]\n",
    "\n",
    "print(f'[DONE] Protocols mask applied on the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06870175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset malicious labeling done with success\n"
     ]
    }
   ],
   "source": [
    "# Labelling the malicious packets\n",
    "import numpy as np\n",
    "\n",
    "df['is_malicious'] = 1\n",
    "df['attack_type'] = 5\n",
    "print(f'[DONE] Dataset malicious labeling done with success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c916d3",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "\n",
    "This section covers all the data cleaning done in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d763dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleting unnecessary fields\n",
      "[REMOVED] frame.encap_type\n",
      "[REMOVED] frame.ignored\n",
      "[REMOVED] frame.interface_id\n",
      "[REMOVED] frame.interface_name\n",
      "[REMOVED] frame.marked\n",
      "[REMOVED] frame.number\n",
      "[REMOVED] frame.offset_shift\n",
      "[REMOVED] frame.protocols\n",
      "[REMOVED] frame.time\n",
      "[REMOVED] frame.time_delta\n",
      "[REMOVED] frame.time_delta_displayed\n",
      "[REMOVED] frame.time_epoch\n",
      "[REMOVED] frame.time_relative\n",
      "[REMOVED] arp.dst.hw_mac\n",
      "[REMOVED] arp.dst.proto_ipv4\n",
      "[REMOVED] arp.hw.size\n",
      "[REMOVED] arp.hw.type\n",
      "[REMOVED] arp.proto.size\n",
      "[REMOVED] arp.proto.type\n",
      "[REMOVED] arp.src.hw_mac\n",
      "[REMOVED] arp.src.proto_ipv4\n",
      "[INFO] Unnecessary fields deleted with success\n"
     ]
    }
   ],
   "source": [
    "# Array: Fields to delete (IDs, Timestamps, Checksums, etc...)\n",
    "fieldsToDelete = [\n",
    "    'frame.encap_type', 'frame.ignored', 'frame.interface_id', 'frame.interface_name', 'frame.marked', \n",
    "    'frame.number', 'frame.offset_shift', 'frame.protocols', 'frame.time', 'frame.time_delta',\n",
    "    'frame.time_delta_displayed', 'frame.time_epoch', 'frame.time_relative', 'arp.dst.hw_mac', 'arp.dst.proto_ipv4',\n",
    "    'arp.hw.size', 'arp.hw.type', 'arp.proto.size', 'arp.proto.type', 'arp.src.hw_mac', 'arp.src.proto_ipv4'\n",
    "]\n",
    "\n",
    "delete_fields(df, fieldsToDelete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d887aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arp.dst.atm_num_e164',\n",
       " 'arp.dst.atm_num_nsap',\n",
       " 'arp.dst.atm_subaddr',\n",
       " 'arp.dst.drarp_error_status',\n",
       " 'arp.dst.hlen',\n",
       " 'arp.dst.htype',\n",
       " 'arp.dst.hw',\n",
       " 'arp.dst.hw_ax25',\n",
       " 'arp.dst.pln',\n",
       " 'arp.dst.proto',\n",
       " 'arp.dst.slen',\n",
       " 'arp.dst.stype',\n",
       " 'arp.duplicate-address-detected',\n",
       " 'arp.duplicate-address-frame',\n",
       " 'arp.isprobe',\n",
       " 'arp.packet-storm-detected',\n",
       " 'arp.seconds-since-duplicate-address-frame',\n",
       " 'arp.src.atm_afi',\n",
       " 'arp.src.atm_afi.unknown',\n",
       " 'arp.src.atm_data_country_code',\n",
       " 'arp.src.atm_data_country_code_group',\n",
       " 'arp.src.atm_e.164_isdn',\n",
       " 'arp.src.atm_e.164_isdn_group',\n",
       " 'arp.src.atm_end_system_identifier',\n",
       " 'arp.src.atm_high_order_dsp',\n",
       " 'arp.src.atm_international_code_designator',\n",
       " 'arp.src.atm_international_code_designator_group',\n",
       " 'arp.src.atm_num_e164',\n",
       " 'arp.src.atm_num_nsap',\n",
       " 'arp.src.atm_rest_of_address',\n",
       " 'arp.src.atm_selector',\n",
       " 'arp.src.atm_subaddr',\n",
       " 'arp.src.hlen',\n",
       " 'arp.src.htype',\n",
       " 'arp.src.hw',\n",
       " 'arp.src.hw_ax25',\n",
       " 'arp.src.pln',\n",
       " 'arp.src.proto',\n",
       " 'arp.src.slen',\n",
       " 'arp.src.stype']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all empty collumns\n",
    "empty_columns = []\n",
    "for column in df.columns:\n",
    "    if df[column].isnull().all():\n",
    "        empty_columns.append(column)\n",
    "list(empty_columns)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25371d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleting empty columns\n",
      "[REMOVED] arp.dst.atm_num_e164\n",
      "[REMOVED] arp.dst.atm_num_nsap\n",
      "[REMOVED] arp.dst.atm_subaddr\n",
      "[REMOVED] arp.dst.drarp_error_status\n",
      "[REMOVED] arp.dst.hlen\n",
      "[REMOVED] arp.dst.htype\n",
      "[REMOVED] arp.dst.hw\n",
      "[REMOVED] arp.dst.hw_ax25\n",
      "[REMOVED] arp.dst.pln\n",
      "[REMOVED] arp.dst.proto\n",
      "[REMOVED] arp.dst.slen\n",
      "[REMOVED] arp.dst.stype\n",
      "[REMOVED] arp.duplicate-address-detected\n",
      "[REMOVED] arp.duplicate-address-frame\n",
      "[REMOVED] arp.isprobe\n",
      "[REMOVED] arp.packet-storm-detected\n",
      "[REMOVED] arp.seconds-since-duplicate-address-frame\n",
      "[REMOVED] arp.src.atm_afi\n",
      "[REMOVED] arp.src.atm_afi.unknown\n",
      "[REMOVED] arp.src.atm_data_country_code\n",
      "[REMOVED] arp.src.atm_data_country_code_group\n",
      "[REMOVED] arp.src.atm_e.164_isdn\n",
      "[REMOVED] arp.src.atm_e.164_isdn_group\n",
      "[REMOVED] arp.src.atm_end_system_identifier\n",
      "[REMOVED] arp.src.atm_high_order_dsp\n",
      "[REMOVED] arp.src.atm_international_code_designator\n",
      "[REMOVED] arp.src.atm_international_code_designator_group\n",
      "[REMOVED] arp.src.atm_num_e164\n",
      "[REMOVED] arp.src.atm_num_nsap\n",
      "[REMOVED] arp.src.atm_rest_of_address\n",
      "[REMOVED] arp.src.atm_selector\n",
      "[REMOVED] arp.src.atm_subaddr\n",
      "[REMOVED] arp.src.hlen\n",
      "[REMOVED] arp.src.htype\n",
      "[REMOVED] arp.src.hw\n",
      "[REMOVED] arp.src.hw_ax25\n",
      "[REMOVED] arp.src.pln\n",
      "[REMOVED] arp.src.proto\n",
      "[REMOVED] arp.src.slen\n",
      "[REMOVED] arp.src.stype\n",
      "[INFO] Empty columns deleted with success\n"
     ]
    }
   ],
   "source": [
    "# Delete and print the empty columns\n",
    "print('[INFO] Deleting empty columns')\n",
    "for column in empty_columns:\n",
    "    df.drop(column, axis = 1, inplace = True)\n",
    "    print(f'[REMOVED] ' + column)\n",
    "print('[INFO] Empty columns deleted with success')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d9d7e",
   "metadata": {},
   "source": [
    "### Pre-Processing & Data Encoding\n",
    "\n",
    "This section covers all the pre-processing and data encoding done in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfad27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Empty Cells Filling: 0\n"
     ]
    }
   ],
   "source": [
    "# Array: Fields to fill with 0\n",
    "fill_0 = [\n",
    "    'arp.isannouncement', 'arp.isgratuitous'\n",
    "]\n",
    "\n",
    "fill_empty_cells(df, fill_0, '0') # Fill: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c0e2139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Z-Score Normalization\n"
     ]
    }
   ],
   "source": [
    "# Array: Fields to apply z-score normalization\n",
    "zScoreNormalization = [\n",
    "    'frame.cap_len', 'frame.len'\n",
    "]\n",
    "\n",
    "for i in zScoreNormalization:\n",
    "    df[i] = df[i].astype(float)\n",
    "\n",
    "zscore_normalization(df, zScoreNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1892449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Convertion to float done with success\n"
     ]
    }
   ],
   "source": [
    "# Converting from dtypes to float\n",
    "for i in df:\n",
    "    df[i] = df[i].astype(float)\n",
    "print(f'[DONE] Convertion to float done with success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5754d293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375055, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replication of rows\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of times to replicate the rows\n",
    "replications = 21700\n",
    "\n",
    "# Randomly select rows from the original DataFrame\n",
    "replicated_rows = df.sample(n=replications, replace=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the replicated rows\n",
    "df = pd.concat([df, replicated_rows], ignore_index=True)\n",
    "\n",
    "# Verify the resulting DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bcaed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Dataset with pre-processing saved with success\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with all the pre-processing and data encoding done\n",
    "df.to_csv(DATASET_PRE_PROCESSING, index=False)\n",
    "print(f'[DONE] Dataset with pre-processing saved with success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
